{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt4Kkh7-r15U"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com/184/messy-vs-clean-room.zip \\\n",
        "  -O /tmp/messy_vs_clean_room.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxEpwiuztDs8",
        "outputId": "d2e54312-4126-4b23-c0b8-a3af2b3e882b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-26 10:24:24--  https://dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com/184/messy-vs-clean-room.zip\n",
            "Resolving dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com (dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com)... 104.18.42.227, 172.64.145.29, 2606:4700:4400::6812:2ae3, ...\n",
            "Connecting to dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com (dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com)|104.18.42.227|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Cookie coming from dicoding-academy-assets.sgp1.cdn.digitaloceanspaces.com attempted to set domain to digitaloceanspaces.com\n",
            "Length: 70392746 (67M) [application/zip]\n",
            "Saving to: ‘/tmp/messy_vs_clean_room.zip’\n",
            "\n",
            "/tmp/messy_vs_clean 100%[===================>]  67.13M   134MB/s    in 0.5s    \n",
            "\n",
            "2024-05-26 10:24:24 (134 MB/s) - ‘/tmp/messy_vs_clean_room.zip’ saved [70392746/70392746]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "local_zip = '/tmp/messy_vs_clean_room.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/images'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')"
      ],
      "metadata": {
        "id": "7sNQFe4VC8CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    shear_range= 0.2,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255)"
      ],
      "metadata": {
        "id": "s62THbUdEAEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=4,\n",
        ")\n",
        "\n",
        "validation_generator =  test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwVHtxJ7Lf6U",
        "outputId": "ec2e3bc9-9561-469c-a0ee-f4a511bab328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 192 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "3bCCUgzhPhxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGc4vELCRGDy",
        "outputId": "a2074a0a-e628-4d3a-f8f4-622c0d75e7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 70, 70, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 35, 35, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 33, 33, 512)       590336    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 512)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 131072)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               67109376  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67793473 (258.61 MB)\n",
            "Trainable params: 67793473 (258.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WkWbeAr7Xzj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=25,\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=5,\n",
        "      verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuoPWbC1X0W0",
        "outputId": "65499ddb-015a-4535-9a04-f2d47f141818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "25/25 - 8s - loss: 0.9997 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 8s/epoch - 326ms/step\n",
            "Epoch 2/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 49ms/step\n",
            "Epoch 3/20\n",
            "25/25 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 79ms/step\n",
            "Epoch 4/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 5/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 47ms/step\n",
            "Epoch 6/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 7/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 53ms/step\n",
            "Epoch 8/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 46ms/step\n",
            "Epoch 9/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 46ms/step\n",
            "Epoch 10/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 55ms/step\n",
            "Epoch 11/20\n",
            "25/25 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 74ms/step\n",
            "Epoch 12/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 13/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 14/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 15/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 16/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 17/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 18/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 19/20\n",
            "25/25 - 1s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 1s/epoch - 45ms/step\n",
            "Epoch 20/20\n",
            "25/25 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 75ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x795172bf5810>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150,150))\n",
        "\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  if classes==0:\n",
        "   print('Bersih')\n",
        "  else:\n",
        "   print('Kotor')"
      ],
      "metadata": {
        "id": "LDTvqvdPYI0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}